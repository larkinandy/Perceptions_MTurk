{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CreateTrainingRandomSample #\n",
    "\n",
    "**Author: ** Andrew Larkin <br>\n",
    "**Date Created: ** October 21st, 2019 <br>\n",
    "**Organization:** Oregon State University, College of Public Health and Human Sciences\n",
    "\n",
    "\n",
    "**Summary** <br>\n",
    "Our initial training dataset sampling plan was based on a 50m sampling grid of Google Street View (GSV) imagery across all urban areas in Washington.  However, the number of images was greater than expected (~3 million).  To reduce the number of images in our training dataset, we decided to use a two-step sampling approach.  \n",
    "\n",
    "1) The number of images sampled from each urban area is proportional to the number of Twin Registry participants living within the urban area (5km buffer).  \n",
    "2) Images will be sampled to maximize the uniform intra-urban distribution of features across the training sample for each urban area in Washington.\n",
    "\n",
    "This script performs the methodology to execute step 1.  From the grid sampling dataset, the number of target images for each urban area is calculated.  For each urban area, a random sample 100x the sample of the target number of images is then taken from the 50m grid of GSV images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import required libraries ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as ps\n",
    "import numpy as np\n",
    "import globalConstants as gConst\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define global constants ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PARENT_FOLDER = gConst.PARENT_FOLDER\n",
    "INPUT_FILE = PARENT_FOLDER + \"Twin_Address_Training_Subset_10_17_19_reduced.csv\"\n",
    "OUTPUT_FILE = PARENT_FOLDER + \"Twin_Proportional_Training_Samples_10_17_19.csv\"\n",
    "COLUMN_NAMES = ['FID','UATYP10','n_twins','prop_twins','n_train','n_test','n_dev','n_download']\n",
    "REGION_FILE = PARENT_FOLDER + \"Twin_Proportional_Training_Samples_10_17_19.csv\"\n",
    "GSV_GRID = PARENT_FOLDER + \"pointsToSampleInt_10_23_19.csv\"\n",
    "N_TRAIN = 1000\n",
    "N_DEV = 100\n",
    "N_TEST = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columnVals = [[] for i in range(len(COLUMN_NAMES))]\n",
    "rawData = ps.read_csv(INPUT_FILE)\n",
    "numSamples = len(rawData['FID'])\n",
    "uniqueFIDs = list(set(rawData['FID']))\n",
    "rawData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate the number of test, train, and dev images for each urban area ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for uniqueFID in uniqueFIDs:\n",
    "    subsetData = rawData.loc[rawData['FID'] == uniqueFID]\n",
    "    nSubsetRecords = len(subsetData['FID'])\n",
    "    propRecords = nSubsetRecords/numSamples\n",
    "    columnVals[COLUMN_NAMES.index('FID')].append(uniqueFID)\n",
    "    columnVals[COLUMN_NAMES.index('UATYP10')].append(list(subsetData['UATYP10'])[0])\n",
    "    columnVals[COLUMN_NAMES.index('n_twins')].append(nSubsetRecords)\n",
    "    columnVals[COLUMN_NAMES.index('prop_twins')].append(propRecords)\n",
    "    columnVals[COLUMN_NAMES.index('n_train')].append(propRecords*N_TRAIN)\n",
    "    columnVals[COLUMN_NAMES.index('n_test')].append(propRecords*N_TEST)\n",
    "    columnVals[COLUMN_NAMES.index('n_dev')].append(propRecords*N_DEV)\n",
    "    columnVals[COLUMN_NAMES.index('n_download')].append(propRecords*100*(N_TRAIN+N_DEV+N_TEST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save urban area image stats to csv ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = ps.DataFrame({\n",
    "    COLUMN_NAMES[0]:columnVals[0],\n",
    "    COLUMN_NAMES[1]:columnVals[1],\n",
    "    COLUMN_NAMES[2]:columnVals[2],\n",
    "    COLUMN_NAMES[3]:columnVals[3],\n",
    "    COLUMN_NAMES[4]:columnVals[4],\n",
    "    COLUMN_NAMES[5]:columnVals[5],\n",
    "    COLUMN_NAMES[6]:columnVals[6],\n",
    "    COLUMN_NAMES[7]:columnVals[7]\n",
    "        })\n",
    "df.to_csv(OUTPUT_FILE,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load urban area metadata from csv ###\n",
    "**Inputs**\n",
    "- **inputFilepath** (str) - absolute filepath of csv file containing urban metadata\n",
    "\n",
    "**Outputs**\n",
    "- **subsetRecords** (pandas dataframe) - loaded records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRegionsToSample(inputFilepath):\n",
    "    rawData = ps.read_csv(inputFilepath)\n",
    "    subsetRecords = rawData[rawData['status']=='id']\n",
    "    return(subsetRecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get number of images to sample for one urban area ###\n",
    "**Inputs**\n",
    "- **FID** (str) - unique urban area id\n",
    "- **regionSampleRates** (pandas dataframe) - metadata for each urban area\n",
    "\n",
    "**Outputs**\n",
    "- **unnamed** (int) - number of images to sample for the urban rea with unique id = FID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNumberToSample(FID,regionSampleRates):\n",
    "    rawDataSubset = regionSampleRates[regionSampleRates['FID'] == FID]\n",
    "    return(list(rawDataSubset['n_download'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a proportional random sample of GSV images from each urban area and save to csv ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region %i has sample size of 0 19\n"
     ]
    }
   ],
   "source": [
    "regionSampleMeta = getRegionsToSample(REGION_FILE)\n",
    "regionIds = list(regionSampleMeta['FID'])\n",
    "sampleGrid = ps.read_csv(GSV_GRID)\n",
    "for region in regionIds:\n",
    "    sampleSize = int(round(list(regionSampleMeta[regionSampleMeta['FID']==region]['n_download'])[0]))\n",
    "    sampleGridSubset = sampleGrid[sampleGrid['NEAR_FID']==region]\n",
    "    numImagesAvailable = len(sampleGridSubset['NEAR_FID'])\n",
    "    sampleSize = min(sampleSize,numImagesAvailable)\n",
    "    if(sampleSize>=1):\n",
    "        sampleGridSubset.sample(n=sampleSize)\n",
    "        sampleGridSubset.to_csv(PARENT_FOLDER + \"GSV_DOWNLOAD_\" + str(region) + \".csv\",index=False)\n",
    "    else:\n",
    "        print(\"region %i has sample size of 0\",region)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
